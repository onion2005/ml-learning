{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise RAG System - Final Working Version\n",
    "Complete RAG system using Amazon Bedrock with Titan v2 embeddings and Claude 3.5 Sonnet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Initialize Bedrock Client\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Create Bedrock runtime client\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='ap-southeast-2',\n",
    "    config=Config(\n",
    "        retries={'max_attempts': 10, 'mode': 'adaptive'}\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Bedrock client initialized successfully\")\n",
    "print(f\"Region: {bedrock._client_config.region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08097d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Set Up Document Processor\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "try:\n",
    "    from langchain_aws import BedrockEmbeddings\n",
    "    print(\"‚úÖ Using updated langchain_aws package\")\n",
    "except ImportError:\n",
    "    from langchain.embeddings import BedrockEmbeddings\n",
    "    print(\"‚ö†Ô∏è Using deprecated BedrockEmbeddings\")\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Try to install and import EPUB loader\n",
    "epub_loader_available = False\n",
    "try:\n",
    "    # Try to import EPUB loader\n",
    "    from langchain.document_loaders import UnstructuredEPubLoader\n",
    "    epub_loader_available = True\n",
    "    print(\"‚úÖ EPUB loader available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è EPUB loader not available - will only support PDF\")\n",
    "\n",
    "# If EPUB loader failed, try to install dependencies\n",
    "if not epub_loader_available:\n",
    "    try:\n",
    "        print(\"üîß Installing EPUB dependencies...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unstructured[epub]\", \"pypandoc\"])\n",
    "        \n",
    "        # Try to download pandoc\n",
    "        import pypandoc\n",
    "        pypandoc.download_pandoc()\n",
    "        \n",
    "        from langchain.document_loaders import UnstructuredEPubLoader\n",
    "        epub_loader_available = True\n",
    "        print(\"‚úÖ EPUB loader installed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not install EPUB support: {e}\")\n",
    "        print(\"üìù Note: Only PDF files will be supported\")\n",
    "\n",
    "# Clean up any existing database directories\n",
    "db_paths = [\"./rag_db_clean\", \"./enterprise_chroma_db\", \"./enterprise_chroma_db_v2\"]\n",
    "for db_path in db_paths:\n",
    "    if os.path.exists(db_path):\n",
    "        try:\n",
    "            shutil.rmtree(db_path)\n",
    "            print(f\"üóëÔ∏è Removed old database: {db_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not remove {db_path}: {e}\")\n",
    "\n",
    "# Create embeddings with Titan v2 (1024 dimensions)\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\",\n",
    "    region_name=\"ap-southeast-2\"\n",
    ")\n",
    "\n",
    "# Test embeddings\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "print(f\"‚úÖ Embeddings working - dimension: {len(test_embedding)}\")\n",
    "\n",
    "# Create fresh ChromaDB client with unique path\n",
    "unique_id = str(uuid.uuid4())[:8]\n",
    "db_path = f\"./rag_db_{unique_id}\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=db_path,\n",
    "    settings=Settings(\n",
    "        anonymized_telemetry=False,\n",
    "        is_persistent=True\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ChromaDB client created at: {db_path}\")\n",
    "if epub_loader_available:\n",
    "    print(\"‚úÖ Document processor ready (supports PDF and EPUB)\")\n",
    "else:\n",
    "    print(\"‚úÖ Document processor ready (PDF only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Process Book and Create Collection with Performance Monitoring\n",
    "import time\n",
    "\n",
    "# Find book files (prioritize PDF for better performance)\n",
    "book_paths = [\n",
    "    '../book.pdf', './book.pdf', 'book.pdf',\n",
    "    '../book.epub', './book.epub', 'book.epub'\n",
    "]\n",
    "book_path = None\n",
    "file_type = None\n",
    "\n",
    "for path in book_paths:\n",
    "    if os.path.exists(path):\n",
    "        book_path = path\n",
    "        file_type = 'epub' if path.endswith('.epub') else 'pdf'\n",
    "        file_size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"üìñ Found {file_type.upper()} at: {path} ({file_size_mb:.1f} MB)\")\n",
    "        break\n",
    "\n",
    "if not book_path:\n",
    "    print(\"‚ùå No book file found!\")\n",
    "    raise FileNotFoundError(\"Please place book.pdf or book.epub in the current directory\")\n",
    "\n",
    "# Load and process book with performance timing\n",
    "load_start_time = time.time()\n",
    "try:\n",
    "    if file_type == 'epub' and epub_loader_available:\n",
    "        print(\"üìö Using EPUB loader (may be slower due to XML parsing overhead)\")\n",
    "        loader = UnstructuredEPubLoader(book_path)\n",
    "    else:\n",
    "        # Fallback to PDF or force PDF if EPUB fails\n",
    "        if file_type == 'epub':\n",
    "            print(\"‚ö†Ô∏è EPUB loader not available, please convert to PDF or install pandoc\")\n",
    "            print(\"üìù To install pandoc: brew install pandoc (macOS) or apt-get install pandoc (Linux)\")\n",
    "            raise FileNotFoundError(\"Please use PDF format or install pandoc for EPUB support\")\n",
    "        print(\"üìÑ Using PDF loader (optimized for performance)\")\n",
    "        loader = PyPDFLoader(book_path)\n",
    "\n",
    "    docs = loader.load()\n",
    "    load_time = time.time() - load_start_time\n",
    "    print(f\"üìÑ Loaded {len(docs)} sections/pages in {load_time:.2f}s ({len(docs)/load_time:.1f} pages/sec)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading {file_type.upper()} file: {e}\")\n",
    "\n",
    "    # Try to find alternative format with timing\n",
    "    alt_paths = ['../book.pdf', './book.pdf', 'book.pdf'] if file_type == 'epub' else ['../book.epub', './book.epub', 'book.epub']\n",
    "    alt_found = False\n",
    "\n",
    "    for alt_path in alt_paths:\n",
    "        if os.path.exists(alt_path):\n",
    "            print(f\"üîÑ Trying alternative format: {alt_path}\")\n",
    "            alt_type = 'pdf' if alt_path.endswith('.pdf') else 'epub'\n",
    "            try:\n",
    "                fallback_start = time.time()\n",
    "                if alt_type == 'pdf':\n",
    "                    loader = PyPDFLoader(alt_path)\n",
    "                    docs = loader.load()\n",
    "                    fallback_time = time.time() - fallback_start\n",
    "                    book_path = alt_path\n",
    "                    file_type = alt_type\n",
    "                    alt_found = True\n",
    "                    print(f\"‚úÖ Successfully loaded {alt_type.upper()}: {len(docs)} pages in {fallback_time:.2f}s\")\n",
    "                    break\n",
    "            except Exception as alt_e:\n",
    "                print(f\"‚ùå Alternative also failed: {alt_e}\")\n",
    "\n",
    "    if not alt_found:\n",
    "        raise Exception(\"Could not load any book format. Please ensure you have a valid PDF file.\")\n",
    "\n",
    "# Split into chunks with timing\n",
    "chunk_start_time = time.time()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "chunk_time = time.time() - chunk_start_time\n",
    "print(f\"üìù Created {len(chunks)} chunks in {chunk_time:.2f}s ({len(chunks)/chunk_time:.1f} chunks/sec)\")\n",
    "\n",
    "# Create collection with unique name\n",
    "collection_name = f\"book_rag_{int(time.time())}\"\n",
    "collection = chroma_client.create_collection(collection_name)\n",
    "print(f\"‚úÖ Created collection: {collection_name}\")\n",
    "\n",
    "# Process in batches with performance monitoring\n",
    "batch_size = 25  # Conservative batch size for all formats\n",
    "total_processed = 0\n",
    "embedding_start_time = time.time()\n",
    "\n",
    "print(f\"üîÑ Processing {len(chunks)} chunks in batches of {batch_size}...\")\n",
    "\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    batch_num = i // batch_size + 1\n",
    "    total_batches = (len(chunks) + batch_size - 1) // batch_size\n",
    "    \n",
    "    batch_start = time.time()\n",
    "    print(f\"   Batch {batch_num}/{total_batches}: {len(batch)} chunks\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        documents = [chunk.page_content for chunk in batch]\n",
    "        metadatas = [{\n",
    "            \"source\": f\"book.{file_type}\",\n",
    "            \"chunk_id\": i + j,\n",
    "            \"page\": getattr(chunk, 'metadata', {}).get('page', 0),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"file_type\": file_type\n",
    "        } for j, chunk in enumerate(batch)]\n",
    "        ids = [f\"chunk_{i + j}_{unique_id}\" for j in range(len(batch))]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        batch_embeddings = embeddings.embed_documents(documents)\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids,\n",
    "            embeddings=batch_embeddings\n",
    "        )\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        total_processed += len(batch)\n",
    "        progress = (total_processed / len(chunks)) * 100\n",
    "        print(f\"({batch_time:.1f}s) ‚úÖ {total_processed}/{len(chunks)} ({progress:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Batch {batch_num} failed: {e}\")\n",
    "        break\n",
    "\n",
    "embedding_time = time.time() - embedding_start_time\n",
    "total_time = time.time() - load_start_time\n",
    "\n",
    "print(f\"\\nüéâ Successfully processed {total_processed} chunks from {file_type.upper()}!\")\n",
    "print(f\"üìä Performance Summary:\")\n",
    "print(f\"   ‚Ä¢ File loading: {load_time:.2f}s\")\n",
    "print(f\"   ‚Ä¢ Text chunking: {chunk_time:.2f}s\") \n",
    "print(f\"   ‚Ä¢ Embedding + storage: {embedding_time:.2f}s\")\n",
    "print(f\"   ‚Ä¢ Total time: {total_time:.2f}s\")\n",
    "print(f\"   ‚Ä¢ Overall rate: {total_processed/total_time:.1f} chunks/sec\")\n",
    "\n",
    "# Verify collection\n",
    "try:\n",
    "    count = collection.count()\n",
    "    print(f\"‚úÖ Collection verification: {count} documents stored\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Collection verification failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: RAG Engine\n",
    "class RAGEngine:\n",
    "    def __init__(self, collection, bedrock_client, embeddings_model):\n",
    "        self.collection = collection\n",
    "        self.bedrock = bedrock_client\n",
    "        self.embeddings = embeddings_model\n",
    "    \n",
    "    def query(self, question: str, k: int = 3):\n",
    "        \"\"\"Ask a question and get an answer with sources\"\"\"\n",
    "        try:\n",
    "            # Generate query embedding with same model as collection\n",
    "            query_embedding = self.embeddings.embed_query(question)\n",
    "            \n",
    "            # Retrieve relevant chunks using the embedding\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=k\n",
    "            )\n",
    "            \n",
    "            if not results['documents'][0]:\n",
    "                return {'answer': 'No relevant context found', 'sources': []}\n",
    "            \n",
    "            # Format context\n",
    "            context = \"\\n\\n\".join([\n",
    "                f\"Source {i+1}: {results['documents'][0][i]}\"\n",
    "                for i in range(len(results['documents'][0]))\n",
    "            ])\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = f\"\"\"Based on the following French context, answer the question in English. \n",
    "            Translate and summarize the relevant information.\n",
    "\n",
    "            Context (in French):\n",
    "            {context}\n",
    "\n",
    "            Question (in English): {question}\n",
    "\n",
    "            Please provide a detailed answer in English:\"\"\"\n",
    "            \n",
    "            # Get answer from Claude 3.5 Sonnet v2\n",
    "            response = self.bedrock.invoke_model(\n",
    "                modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
    "                body=json.dumps({\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"max_tokens\": 500,\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "                }),\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response['body'].read())\n",
    "            answer = result['content'][0]['text']\n",
    "            \n",
    "            # Format sources\n",
    "            sources = [{\n",
    "                'content': results['documents'][0][i][:200] + \"...\" if len(results['documents'][0][i]) > 200 else results['documents'][0][i],\n",
    "                'metadata': results['metadatas'][0][i]\n",
    "            } for i in range(len(results['documents'][0]))]\n",
    "            \n",
    "            return {'answer': answer, 'sources': sources}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'answer': f'Error: {e}', 'sources': []}\n",
    "\n",
    "# Create RAG engine\n",
    "rag = RAGEngine(collection, bedrock, embeddings)\n",
    "print(\"‚úÖ RAG Engine ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecf3f68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing RAG System...\n",
      "\n",
      "================================================================================\n",
      "ü§î QUESTION: Provide me a summary of this book, chapter by chapter\n",
      "================================================================================\n",
      "\n",
      "üí° ANSWER:\n",
      "Based on the given French context, I cannot provide a complete chapter-by-chapter summary of the book, as the excerpts only show introductory or concluding remarks about different sections. However, I can tell you what these passages reveal about the book's structure and themes:\n",
      "\n",
      "1. The book appears to contain three main parts:\n",
      "\n",
      "- A section about the United States (Source 1)\n",
      "The author apologizes for the schematic nature of three chapters devoted to the US, acknowledging that the crisis in American society is complex enough to warrant its own book. The author prioritizes timely analysis over academic perfection due to ongoing war-related developments.\n",
      "\n",
      "- A section about geopolitical relations (Source 2)\n",
      "This appears to be a concluding section that chronologically analyzes the thirty years following the Cold War, focusing on how NATO became entangled in the \"Ukrainian trap.\" It examines the internal evolution of Russian, Ukrainian, Eastern European, and Western societies.\n",
      "\n",
      "- A section on religious and societal matters (Source 3)\n",
      "This part explores the religious foundations of societies, human responses to existential questions, and particularly examines the dissolution of the Christian religious matrix in the West, especially its Protestant variant. It introduces the concept of \"religious zero state\" and discusses nihilism.\n",
      "\n",
      "Without access to the complete book, I cannot provide a detailed chapter-by-chapter summary. These excerpts suggest it's a work analyzing contemporary geopolitical, societal, and religious crises, particularly focusing on the United States, NATO, and Western civilization's religious foundations.\n",
      "\n",
      "üìö SOURCES:\n",
      "\n",
      "1. Chunk 526 (Page 0)\n",
      "   Je prie d‚Äôavance le lecteur d‚Äôexcuser le caract√®re sch√©matique des trois chapitres qui vont √™tre consacr√©s aux √âtats-Unis. Tout n‚Äôy sera pas d√©montr√©. La crise d‚Äôune soci√©t√© si complexe devrait faire ...\n",
      "\n",
      "2. Chunk 738 (Page 0)\n",
      "   Nous avons √† plusieurs reprises retrac√© les cons√©quences de cette illusion, mais en ordre dispers√©. Il est temps, pour clore ce livre, de rassembler en une s√©quence ordonn√©e par la chronologie les √©l√©...\n",
      "\n",
      "3. Chunk 55 (Page 0)\n",
      "   Les chapitres qui suivent traiteront donc aussi de la matrice religieuse des soci√©t√©s, des solutions que l‚Äôhomme s‚Äôest efforc√© de trouver au myst√®re de sa condition et √† son caract√®re difficilement ac...\n",
      "================================================================================\n",
      "\n",
      "\n",
      "‚úÖ RAG system is working! You can now use ask_question('your question') to query the book.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test the RAG System\n",
    "def ask_question(question, k=3):\n",
    "    \"\"\"Ask a question and display the answer nicely\"\"\"\n",
    "    result = rag.query(question, k=k)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ü§î QUESTION: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüí° ANSWER:\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    if result['sources']:\n",
    "        print(f\"\\nüìö SOURCES:\")\n",
    "        for i, source in enumerate(result['sources'], 1):\n",
    "            print(f\"\\n{i}. Chunk {source['metadata']['chunk_id']} (Page {source['metadata']['page']})\")\n",
    "            print(f\"   {source['content']}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    return result\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"Provide me a summary of this book, chapter by chapter\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing RAG System...\\n\")\n",
    "\n",
    "for question in test_questions:\n",
    "    ask_question(question)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"‚úÖ RAG system is working! You can now use ask_question('your question') to query the book.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pq4iemmnmgm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Comparison and Optimization Tips\n",
    "def compare_formats():\n",
    "    \"\"\"Compare PDF vs EPUB processing performance\"\"\"\n",
    "    print(\"üìä PDF vs EPUB Performance Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"üîç Why EPUB might be slower:\")\n",
    "    print(\"‚Ä¢ UnstructuredEPubLoader parses XML/XHTML structure\")\n",
    "    print(\"‚Ä¢ Extracts from multiple files within EPUB archive\") \n",
    "    print(\"‚Ä¢ Converts complex formatting to plain text\")\n",
    "    print(\"‚Ä¢ Requires pandoc and additional dependencies\")\n",
    "    print(\"‚Ä¢ More metadata processing overhead\")\n",
    "    \n",
    "    print(\"\\n‚ö° Why PDF is often faster:\")\n",
    "    print(\"‚Ä¢ PyPDFLoader is highly optimized\")\n",
    "    print(\"‚Ä¢ Direct binary format parsing\")\n",
    "    print(\"‚Ä¢ Mature codebase with performance optimizations\")\n",
    "    print(\"‚Ä¢ Less dependency overhead\")\n",
    "    \n",
    "    print(\"\\nüöÄ Optimization recommendations:\")\n",
    "    print(\"‚Ä¢ Use PDF format for best performance\")\n",
    "    print(\"‚Ä¢ Consider pre-processing EPUB to text files\")\n",
    "    print(\"‚Ä¢ Increase batch_size if you have more memory\")\n",
    "    print(\"‚Ä¢ Use parallel processing for very large documents\")\n",
    "    \n",
    "    # Show current settings\n",
    "    print(f\"\\n‚öôÔ∏è Current settings:\")\n",
    "    print(f\"‚Ä¢ Batch size: 25 chunks\")\n",
    "    print(f\"‚Ä¢ Chunk size: 1000 characters\")\n",
    "    print(f\"‚Ä¢ Chunk overlap: 200 characters\")\n",
    "    print(f\"‚Ä¢ Embedding model: amazon.titan-embed-text-v2:0 (1024 dims)\")\n",
    "\n",
    "compare_formats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
